from base import pipeline, clean_db
from collections import namedtuple
import getpass
import os
import psycopg2
import psycopg2.extensions
import random
import threading
import time


DURATION = 2 * 60 # 2 minutes

Stat = namedtuple('Stat', ['rows', 'matrel_rows', 'disk_pages'])


def test_disk_spill(pipeline, clean_db):
  pipeline.create_cv(
    'test_vacuum', '''
    SELECT x::int, COUNT(DISTINCT y::text)
    FROM test_vacuum_stream
    WHERE arrival_timestamp > clock_timestamp() - INTERVAL '5 second'
    GROUP BY x
    ''')
  conn = psycopg2.connect('dbname=pipeline user=%s host=localhost port=%s' %
                          (getpass.getuser(), pipeline.port))
  conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
  db = conn.cursor()

  # VACUUM FULL and ANALYZE everything so our disk page usage is minimal +
  # accurate.
  db.execute('VACUUM FULL')
  db.execute('ANALYZE')

  stop = False

  def insert():
    while not stop:
      values = map(lambda _: (random.randint(0, 20),
                              random.randint(0, 1000000)),
                   xrange(1000))
      pipeline.insert('test_vacuum_stream', ('x', 'y'), values)
      time.sleep(0.01)

  t = threading.Thread(target=insert)
  t.start()

  # Wait for SW AV to kick in
  time.sleep(30)

  def get_stat():
    rows = pipeline.execute(
      'SELECT COUNT(*) FROM test_vacuum').first()['count']
    matrel_rows = pipeline.execute(
      'SELECT COUNT(*) FROM test_vacuum_mrel').first()['count']
    disk_pages = pipeline.execute("""
    SELECT pg_relation_filepath(oid), relpages
    FROM pg_class WHERE relname = 'test_vacuum_mrel';
    """).first()['relpages']
    for r in pipeline.execute("""
    SELECT relname, relpages
    FROM pg_class,
         (SELECT reltoastrelid
          FROM pg_class
          WHERE relname = 'test_vacuum_mrel') AS ss
    WHERE oid = ss.reltoastrelid OR
          oid = (SELECT indexrelid
                 FROM pg_index
                 WHERE indrelid = ss.reltoastrelid)
    ORDER BY relname;
    """):
      disk_pages += r['relpages']
    for r in pipeline.execute("""
    SELECT c2.relname, c2.relpages
    FROM pg_class c, pg_class c2, pg_index i
    WHERE c.relname = 'test_vacuum_mrel' AND
          c.oid = i.indrelid AND
          c2.oid = i.indexrelid
    ORDER BY c2.relname;
    """):
      disk_pages += r['relpages']

    return Stat(rows, matrel_rows, disk_pages)

  start = time.time()
  stats = []

  pipeline.execute('ANALYZE test_vacuum')

  while time.time() - start < DURATION:
    stats.append(get_stat())
    time.sleep(10)

  stop = True
  t.join()

  # We should always see the same number of rows
  rows = map(lambda s: s.rows, stats)
  assert len(set(rows)) == 1

  # The number of matrel rows should go up and down
  matrel_rows = map(lambda s: s.matrel_rows, stats)
  assert sorted(matrel_rows) != matrel_rows

  # TODO(usmanm): This check fails of CircleCI, so skip for now.
  if os.environ.get('iamcircle') is None:
    # The number of disk pages should also go up and down
    disk_pages = map(lambda s: s.disk_pages, stats)
    assert sorted(disk_pages) != disk_pages

  db.execute('ANALYZE')
  before = get_stat()

  # Now vacuum while we're not inserting at all and see if it does a major
  # clean up
  # We need to VACUUM twice to really *free* up the heap space used by
  # expired SW tuples. In the first pass, they're marked as deleted and
  # in the second pass, they're physical space is marked as re-usable.
  db.execute('VACUUM ANALYZE test_vacuum')
  db.execute('VACUUM ANALYZE test_vacuum')

  db.execute('ANALYZE')
  after = get_stat()

  assert after.matrel_rows < before.matrel_rows
  assert after.disk_pages <= before.disk_pages

  conn.close()


def test_concurrent_vacuum_full(pipeline, clean_db):
  pipeline.create_cv(
    'test_vacuum_full',
    'SELECT x::int, COUNT(*) FROM test_vacuum_stream GROUP BY x')
  stop = False

  def insert():
    while not stop:
      values = [(random.randint(0, 1000000), ) for _ in xrange(1000)]
      pipeline.insert('test_vacuum_stream', ('x', ), values)
      time.sleep(0.01)

  threads = [threading.Thread(target=insert) for _ in range(4)]
  map(lambda t: t.start(), threads)

  # Insert data for a little bit so we have enough work to do while
  # vacuuming.
  time.sleep(20)

  conn = psycopg2.connect('dbname=pipeline user=%s host=localhost port=%s' %
                          (getpass.getuser(), pipeline.port))
  conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
  cur = conn.cursor()
  cur.execute('VACUUM FULL test_vacuum_full')
  conn.close()

  # Now kill the insert threads.
  stop = True
  map(lambda t: t.join(), threads)
